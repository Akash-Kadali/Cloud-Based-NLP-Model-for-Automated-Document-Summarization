{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install blinker==1.9.0\n",
        "!pip install certifi==2024.8.30\n",
        "!pip install cffi==1.17.1\n",
        "!pip install charset-normalizer==3.4.0\n",
        "!pip install click==8.1.7\n",
        "!pip install cryptography==44.0.0\n",
        "!pip install filelock==3.16.1\n",
        "!pip install Flask==3.1.0\n",
        "!pip install Flask-Cors==5.0.0\n",
        "!pip install fsspec==2024.10.0\n",
        "!pip install huggingface-hub==0.26.5\n",
        "!pip install idna==3.10\n",
        "!pip install itsdangerous==2.2.0\n",
        "!pip install Jinja2==3.1.4\n",
        "!pip install MarkupSafe==3.0.2\n",
        "!pip install mpmath==1.3.0\n",
        "!pip install networkx==3.4.2\n",
        "!pip install numpy==2.1.3\n",
        "!pip install packaging==24.2\n",
        "!pip install pdfminer.six==20231228\n",
        "!pip install pdfplumber==0.11.4\n",
        "!pip install pillow==11.0.0\n",
        "!pip install pycparser==2.22\n",
        "!pip install pypdfium2==4.30.0\n",
        "!pip install PyYAML==6.0.2\n",
        "!pip install regex==2024.11.6\n",
        "!pip install requests==2.32.3\n",
        "!pip install safetensors==0.4.5\n",
        "!pip install setuptools==75.6.0\n",
        "!pip install sympy==1.13.1\n",
        "!pip install tokenizers==0.21.0\n",
        "!pip install torch==2.5.1\n",
        "!pip install tqdm==4.67.1\n",
        "!pip install transformers==4.47.0\n",
        "!pip install typing_extensions==4.12.2\n",
        "!pip install urllib3==2.2.3\n",
        "!pip install Werkzeug==3.1.3\n",
        "\"\"\"\n",
        "Document Summarization API\n",
        "\n",
        "This Flask application provides an API endpoint for summarizing PDF and text documents.\n",
        "It uses the BART-large-CNN model for text summarization and supports both PDF and TXT files.\n",
        "\n",
        "Key Features:\n",
        "- File upload handling\n",
        "- PDF and TXT file processing\n",
        "- Text extraction\n",
        "- Automatic text summarization\n",
        "- Error handling and validation\n",
        "- Temporary file management\n",
        "\n",
        "Dependencies:\n",
        "- Flask: Web framework\n",
        "- flask_cors: Cross-origin resource sharing\n",
        "- transformers: Hugging Face transformers for summarization\n",
        "- pdfplumber: PDF text extraction\n",
        "\"\"\"\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from transformers import pipeline\n",
        "import pdfplumber\n",
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "# Application Configuration\n",
        "MAX_TEXT_LENGTH = 1024  # Maximum number of characters to process\n",
        "MAX_SUMMARY_LENGTH = 130  # Maximum length of generated summary\n",
        "MIN_SUMMARY_LENGTH = 30  # Minimum length of generated summary\n",
        "ALLOWED_EXTENSIONS = {'pdf', 'txt'}  # Supported file types\n",
        "TEMP_DIR = \"temp\"  # Directory for temporary file storage\n",
        "\n",
        "# Initialize Flask application and CORS\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Initialize the summarization model\n",
        "# Using facebook/bart-large-cnn model which is optimized for news article summarization\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def is_valid_file_extension(filename: str) -> bool:\n",
        "    \"\"\"\n",
        "    Validate if the uploaded file has an allowed extension.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Name of the uploaded file\n",
        "\n",
        "    Returns:\n",
        "        bool: True if file extension is allowed, False otherwise\n",
        "\n",
        "    Example:\n",
        "        >>> is_valid_file_extension(\"document.pdf\")\n",
        "        True\n",
        "        >>> is_valid_file_extension(\"document.doc\")\n",
        "        False\n",
        "    \"\"\"\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "def extract_text_from_file(file_path: str, file_extension: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extract text content from the uploaded file based on its type.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the temporary stored file\n",
        "        file_extension (str): Extension of the file ('pdf' or 'txt')\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: Extracted text content or None if extraction fails\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If text extraction encounters an error\n",
        "\n",
        "    Example:\n",
        "        >>> text = extract_text_from_file(\"/temp/doc.pdf\", \"pdf\")\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if file_extension == 'pdf':\n",
        "            # Handle PDF files using pdfplumber\n",
        "            with pdfplumber.open(file_path) as pdf_document:\n",
        "                # Extract text from each page and join with spaces\n",
        "                return \" \".join(page.extract_text() or \"\" for page in pdf_document.pages).strip()\n",
        "        else:  # txt file\n",
        "            # Handle plain text files\n",
        "            with open(file_path, 'r', encoding='utf-8') as text_file:\n",
        "                return text_file.read().strip()\n",
        "    except Exception as error:\n",
        "        raise ValueError(f\"Error extracting text: {str(error)}\")\n",
        "\n",
        "@app.route('/summarize', methods=['POST'])\n",
        "def summarize_document():\n",
        "    \"\"\"\n",
        "    API endpoint to handle document upload and generate summary.\n",
        "\n",
        "    Expected input: Multipart form data with a 'file' field containing the document\n",
        "\n",
        "    Returns:\n",
        "        JSON response containing:\n",
        "        - summary: Generated summary text\n",
        "        - original_length: Length of original text\n",
        "        - summary_length: Length of generated summary\n",
        "        - error: Error message if processing fails\n",
        "\n",
        "    HTTP Status Codes:\n",
        "        200: Success\n",
        "        400: Invalid request (missing file, invalid type)\n",
        "        500: Server error during processing\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Step 1: Validate file upload\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({\"error\": \"No file uploaded\"}), 400\n",
        "\n",
        "        uploaded_file = request.files['file']\n",
        "        if not uploaded_file or not uploaded_file.filename:\n",
        "            return jsonify({\"error\": \"Invalid file\"}), 400\n",
        "\n",
        "        # Step 2: Validate file extension\n",
        "        if not is_valid_file_extension(uploaded_file.filename):\n",
        "            return jsonify({\"error\": f\"Unsupported file type. Allowed types: {', '.join(ALLOWED_EXTENSIONS)}\"}), 400\n",
        "\n",
        "        # Step 3: Set up temporary storage\n",
        "        os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "        temporary_file_path = os.path.join(TEMP_DIR, uploaded_file.filename)\n",
        "\n",
        "        try:\n",
        "            # Step 4: Save and process the uploaded file\n",
        "            uploaded_file.save(temporary_file_path)\n",
        "            file_extension = uploaded_file.filename.rsplit('.', 1)[1].lower()\n",
        "            extracted_text = extract_text_from_file(temporary_file_path, file_extension)\n",
        "\n",
        "            # Step 5: Validate extracted text\n",
        "            if not extracted_text:\n",
        "                return jsonify({\"error\": \"No text could be extracted from the file\"}), 400\n",
        "\n",
        "            # Step 6: Truncate text if it exceeds maximum length\n",
        "            if len(extracted_text) > MAX_TEXT_LENGTH:\n",
        "                extracted_text = extracted_text[:MAX_TEXT_LENGTH]\n",
        "\n",
        "            # Step 7: Generate summary using the BART model\n",
        "            generated_summary = summarizer(\n",
        "                extracted_text,\n",
        "                max_length=MAX_SUMMARY_LENGTH,\n",
        "                min_length=MIN_SUMMARY_LENGTH,\n",
        "                do_sample=False  # Deterministic generation\n",
        "            )[0]['summary_text']\n",
        "\n",
        "            # Step 8: Return successful response with summary\n",
        "            return jsonify({\n",
        "                \"summary\": generated_summary,\n",
        "                \"original_length\": len(extracted_text),\n",
        "                \"summary_length\": len(generated_summary)\n",
        "            })\n",
        "\n",
        "        finally:\n",
        "            # Step 9: Clean up - Remove temporary file\n",
        "            if os.path.exists(temporary_file_path):\n",
        "                os.remove(temporary_file_path)\n",
        "\n",
        "    except Exception as error:\n",
        "        # Step 10: Handle any unexpected errors\n",
        "        return jsonify({\n",
        "            \"error\": \"An error occurred while processing the file\",\n",
        "            \"details\": str(error)\n",
        "        }), 500\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run the Flask application in debug mode when executed directly\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "AaV-z3DRuyo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(example):\n",
        "    model_input = tokenizer(example[\"article\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example[\"highlights\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_input\n",
        "\n",
        "tokenized = dataset[\"train\"].map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# Training config\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bart-cnn-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=2,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=1,  # Change to 3–5+ for better results\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        ")\n",
        "\n",
        "# Train and save\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./bart-cnn-finetuned\")\n",
        "tokenizer.save_pretrained(\"./bart-cnn-finetuned\")"
      ],
      "metadata": {
        "id": "vzikEA5Ytixn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess(example):\n",
        "    model_input = tokenizer(example[\"article\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(example[\"highlights\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "    model_input[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_input\n",
        "\n",
        "tokenized = dataset[\"train\"].map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "\n",
        "# Training config\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bart-cnn-finetuned\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=2,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=1,  # Change to 3–5+ for better results\n",
        "    fp16=True,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        ")\n",
        "\n",
        "# Train and save\n",
        "trainer.train()\n",
        "model.save_pretrained(\"./bart-cnn-finetuned\")\n",
        "tokenizer.save_pretrained(\"./bart-cnn-finetuned\")"
      ],
      "metadata": {
        "id": "D7EiNIHbtiz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Document Summarization API using Fine-Tuned BART\n",
        "\n",
        "This Flask app summarizes PDF or TXT documents using a fine-tuned\n",
        "facebook/bart-large-cnn model on the CNN/DailyMail dataset.\n",
        "\n",
        "Dependencies:\n",
        "- Flask\n",
        "- flask_cors\n",
        "- transformers\n",
        "- pdfplumber\n",
        "\"\"\"\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from transformers import pipeline\n",
        "import pdfplumber\n",
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "# ---------------------------- Configuration ----------------------------\n",
        "MAX_TEXT_LENGTH = 1024\n",
        "MAX_SUMMARY_LENGTH = 130\n",
        "MIN_SUMMARY_LENGTH = 30\n",
        "ALLOWED_EXTENSIONS = {'pdf', 'txt'}\n",
        "TEMP_DIR = \"temp\"\n",
        "\n",
        "# ---------------------------- App Setup ----------------------------\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Load the fine-tuned model from local directory\n",
        "MODEL_DIR = \"./bart-cnn-finetuned\"  # Make sure this path contains your trained model\n",
        "summarizer = pipeline(\"summarization\", model=MODEL_DIR, tokenizer=MODEL_DIR)\n",
        "\n",
        "# ---------------------------- Utility Functions ----------------------------\n",
        "def is_valid_file_extension(filename: str) -> bool:\n",
        "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
        "\n",
        "def extract_text_from_file(file_path: str, file_extension: str) -> Optional[str]:\n",
        "    try:\n",
        "        if file_extension == 'pdf':\n",
        "            with pdfplumber.open(file_path) as pdf:\n",
        "                return \" \".join(page.extract_text() or \"\" for page in pdf.pages).strip()\n",
        "        else:\n",
        "            with open(file_path, 'r', encoding='utf-8') as text_file:\n",
        "                return text_file.read().strip()\n",
        "    except Exception as error:\n",
        "        raise ValueError(f\"Error extracting text: {str(error)}\")\n",
        "\n",
        "# ---------------------------- API Endpoint ----------------------------\n",
        "@app.route('/summarize', methods=['POST'])\n",
        "def summarize_document():\n",
        "    try:\n",
        "        if 'file' not in request.files:\n",
        "            return jsonify({\"error\": \"No file uploaded\"}), 400\n",
        "\n",
        "        uploaded_file = request.files['file']\n",
        "        if not uploaded_file or not uploaded_file.filename:\n",
        "            return jsonify({\"error\": \"Invalid file\"}), 400\n",
        "\n",
        "        if not is_valid_file_extension(uploaded_file.filename):\n",
        "            return jsonify({\n",
        "                \"error\": f\"Unsupported file type. Allowed types: {', '.join(ALLOWED_EXTENSIONS)}\"\n",
        "            }), 400\n",
        "\n",
        "        os.makedirs(TEMP_DIR, exist_ok=True)\n",
        "        temp_path = os.path.join(TEMP_DIR, uploaded_file.filename)\n",
        "\n",
        "        try:\n",
        "            uploaded_file.save(temp_path)\n",
        "            ext = uploaded_file.filename.rsplit('.', 1)[1].lower()\n",
        "            text = extract_text_from_file(temp_path, ext)\n",
        "\n",
        "            if not text:\n",
        "                return jsonify({\"error\": \"No text could be extracted from the file\"}), 400\n",
        "\n",
        "            if len(text) > MAX_TEXT_LENGTH:\n",
        "                text = text[:MAX_TEXT_LENGTH]\n",
        "\n",
        "            summary = summarizer(\n",
        "                text,\n",
        "                max_length=MAX_SUMMARY_LENGTH,\n",
        "                min_length=MIN_SUMMARY_LENGTH,\n",
        "                do_sample=False\n",
        "            )[0]['summary_text']\n",
        "\n",
        "            return jsonify({\n",
        "                \"summary\": summary,\n",
        "                \"original_length\": len(text),\n",
        "                \"summary_length\": len(summary)\n",
        "            })\n",
        "\n",
        "        finally:\n",
        "            if os.path.exists(temp_path):\n",
        "                os.remove(temp_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            \"error\": \"An error occurred while processing the file\",\n",
        "            \"details\": str(e)\n",
        "        }), 500\n",
        "\n",
        "# ---------------------------- Run ----------------------------\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)"
      ],
      "metadata": {
        "id": "CLuJXLQ5ti2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ULoId89rti56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}